{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11869742,"sourceType":"datasetVersion","datasetId":7459206},{"sourceId":12537171,"sourceType":"datasetVersion","datasetId":7914863},{"sourceId":12564627,"sourceType":"datasetVersion","datasetId":7934346}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\nfrom scipy.fftpack import fft\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.signal import butter, filtfilt, resample_poly\nfrom fractions import Fraction\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nfrom tensorflow.keras.models import load_model\n\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\n\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\nfrom itertools import cycle\n\nimport seaborn as sns\n\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg\nfrom PIL import Image\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#File names\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Load a model\n\nloaded_model = load_model(\"\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# Input Dataset I","metadata":{}},{"cell_type":"code","source":"\ndata0D = pd.read_csv('/kaggle/input/vibration-dataset-1/0D.csv')\ndata1D = pd.read_csv('/kaggle/input/vibration-dataset-1/1D.csv')\ndata2D = pd.read_csv('/kaggle/input/vibration-dataset-1/2D.csv')\ndata3D = pd.read_csv('/kaggle/input/vibration-dataset-1/3D.csv')\ndata4D = pd.read_csv('/kaggle/input/vibration-dataset-1/4D.csv')\n\ndata0E = pd.read_csv('/kaggle/input/vibration-dataset-1/0E.csv')\ndata1E = pd.read_csv('/kaggle/input/vibration-dataset-1/1E.csv')\ndata2E = pd.read_csv('/kaggle/input/vibration-dataset-1/2E.csv')\ndata3E = pd.read_csv('/kaggle/input/vibration-dataset-1/3E.csv')\ndata4E = pd.read_csv('/kaggle/input/vibration-dataset-1/4E.csv')\n\n\nprint('DATASET LOADED')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The signal was initialized from 20 seconds onward to eliminate the initial transient phase.\n\nfs = 4096\ninitial_time = 20 * fs  \n\n\n# Reset index\ndata0D = data0D.iloc[initial_time:].reset_index(drop=True)\ndata1D = data1D.iloc[initial_time:].reset_index(drop=True)\ndata2D = data2D.iloc[initial_time:].reset_index(drop=True)\ndata3D = data3D.iloc[initial_time:].reset_index(drop=True)\ndata4D = data4D.iloc[initial_time:].reset_index(drop=True)\n\n\ndata0E = data0E.iloc[initial_time:].reset_index(drop=True)\ndata1E = data1E.iloc[initial_time:].reset_index(drop=True)\ndata2E = data2E.iloc[initial_time:].reset_index(drop=True)\ndata3E = data3E.iloc[initial_time:].reset_index(drop=True)\ndata4E = data4E.iloc[initial_time:].reset_index(drop=True)\n\n\nprint('Done')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# One-second window\n\nwindow_time = 1\nwindow = fs * window_time  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extracts signal segments of the specified window size.\n\ndef get_features(data, label):\n    n = int(np.floor(len(data)/window))\n    data = data[:int(n)*window]\n    X = data.values.reshape((n, window))\n    y = np.ones(n)*labels[label]\n    return X,y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = {'no_unbalance':0, 'unbalance_1':1, 'unbalance_2':2,'unbalance_3':3, 'unbalance_4':4}\nsensor = 'Vibration_2'\n\n\nX0D, y0D = get_features(data0D[sensor], \"no_unbalance\")\nX1D, y1D = get_features(data1D[sensor], \"unbalance_1\")\nX2D, y2D = get_features(data2D[sensor], \"unbalance_2\")\nX3D, y3D = get_features(data3D[sensor], \"unbalance_3\")\nX4D, y4D = get_features(data4D[sensor], \"unbalance_4\")\n\n\nX0E, y0E = get_features(data0E[sensor], \"no_unbalance\")\nX1E, y1E = get_features(data1E[sensor], \"unbalance_1\")\nX2E, y2E = get_features(data2E[sensor], \"unbalance_2\")\nX3E, y3E = get_features(data3E[sensor], \"unbalance_3\")\nX4E, y4E = get_features(data4E[sensor], \"unbalance_4\")\n\n\nX=np.concatenate([X0D, X1D, X2D, X3D, X4D, X0E, X1E, X2E, X3E, X4E])\nY=np.concatenate([y0D, y1D, y2D, y3D, y4D, y0E, y1E, y2E, y3E, y4E])\n\n\nprint(X.shape, Y.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Histogram used to visualize the number of samples per class (to verify whether the dataset is balanced).\n\ncustom_labels = [\n    \"Normal\",\n    \"Unb. I\",\n    \"Unb. II\",\n    \"Unb. III\",\n    \"Unb. IV\"\n]\n\n\nplt.figure(figsize=(8,5))\nplt.hist(Y, bins=np.arange(len(labels)+1)-0.5, edgecolor='black', rwidth=0.8)\n\n\nplt.xticks(range(len(labels)), custom_labels, rotation=0)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Absolute Frequency\")\nplt.title(\"Class Distribution for Dataset I\")\n\nplt.grid(axis='y', linestyle='--', alpha=0.6)\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# Input Dataset II","metadata":{}},{"cell_type":"code","source":"\n\nfor i in range(1, 1001):\n    globals()[f\"data_normal_{i}\"] = pd.read_csv(f'/kaggle/input/vbl-va001/normal/normal_{i}.csv', header=None)\n\nfor i in range(1, 501):\n    globals()[f\"data_unbalance_i_{i}\"] = pd.read_csv(f'/kaggle/input/vbl-va001/unbalance_6/unbalance_i_{i}.csv', header=None)\n\nfor i in range(1, 501):\n    globals()[f\"data_unbalance_ii_{i}\"] = pd.read_csv(f'/kaggle/input/vbl-va001/unbalance_27/unbalance_ii_{i}.csv', header=None)\n\n\n\nfor i in range(1, 1001):\n    globals()[f\"data_misalignment_{i}\"] = pd.read_csv(f'/kaggle/input/vbl-va001/misalignment/misalignment_{i}.csv', header=None)\n    \n\nfor i in range(1, 1001):\n    globals()[f\"data_bearing_{i}\"] = pd.read_csv(f'/kaggle/input/vbl-va001/bearing/bearing_{i}.csv', header=None)\n\n\n\nprint('DATASET LOADED')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize which axis exhibits the highest amplitude.\n\n\ndf = globals()[\"data_unbalance_ii_100\"]\n\ntime = df.iloc[:, 0]\naxis_x = df.iloc[:, 1]\naxis_y = df.iloc[:, 2]\naxis_z = df.iloc[:, 3]\n\nplt.figure(figsize=(12, 6))\nplt.plot(time, axis_x, label='X axis')\nplt.plot(time, axis_y, label='Y axis')\nplt.plot(time, axis_z, label='Z axis')\nplt.title('Vibration in 3 axis - Unbalance II')\nplt.xlabel('Time (s)')\nplt.ylabel('Amplitude')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fs = 20000\n\nwindow_time = 1\nwindow = fs * window_time  \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extracts signal segments of the specified window size.\n\ndef get_features(data, label):\n    n = int(np.floor(len(data)/window))\n    data = data[:int(n)*window]\n    X = data.values.reshape((n, window))\n    y = np.ones(n)*labels[label]\n    return X,y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = {'normal':0, 'unbalance_1':1, 'unbalance_2':2,'misaligment':3, 'bearing_fault':4}\naxis = 2\n\n\nX_list = []\nY_list = []\n\n\nfor i in range(1, 1001):\n    globals()[f'X_normal_{i}'], globals()[f'y_normal_{i}'] = get_features(globals()[f'data_normal_{i}'][axis], \"normal\")\n    X_list.append(globals()[f\"X_normal_{i}\"])\n    Y_list.append(globals()[f\"y_normal_{i}\"])\n\n\nfor i in range(1, 501):\n    globals()[f'X_unbalance_i_{i}'], globals()[f'y_unbalance_i_{i}'] = get_features(globals()[f'data_unbalance_i_{i}'][axis], \"unbalance_1\")\n    X_list.append(globals()[f\"X_unbalance_i_{i}\"])\n    Y_list.append(globals()[f\"y_unbalance_i_{i}\"])\n    \nfor i in range(1, 501):\n    globals()[f'X_unbalance_ii_{i}'], globals()[f'y_unbalance_ii_{i}'] = get_features(globals()[f'data_unbalance_ii_{i}'][axis], \"unbalance_2\")\n    X_list.append(globals()[f\"X_unbalance_ii_{i}\"])\n    Y_list.append(globals()[f\"y_unbalance_ii_{i}\"])\n\n\n\nfor i in range(1, 1001):\n    globals()[f'X_misaligment_{i}'], globals()[f'y_misaligment_{i}'] = get_features(globals()[f'data_misalignment_{i}'][axis], \"misaligment\")\n    X_list.append(globals()[f\"X_misaligment_{i}\"])\n    Y_list.append(globals()[f\"y_misaligment_{i}\"])\n\nfor i in range(1, 1001):\n    globals()[f'X_bearing_{i}'], globals()[f'y_bearing_{i}'] = get_features(globals()[f'data_bearing_{i}'][axis], \"bearing_fault\")\n    X_list.append(globals()[f\"X_bearing_{i}\"])\n    Y_list.append(globals()[f\"y_bearing_{i}\"])\n\n\n\nX=np.concatenate(X_list)\nY=np.concatenate(Y_list)\n\n\nprint(X.shape, Y.shape)\n\nprint('Done')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Histogram used to visualize the number of samples per class (to verify whether the dataset is balanced).\n\ncustom_labels = [\n    \"Normal\",\n    \"Unb. I\",\n    \"Unb. II\",\n    \"Misalig.\",\n    \"Bearings\"\n]\n\n# Criar histograma\nplt.figure(figsize=(8,5))\nplt.hist(Y, bins=np.arange(len(labels)+1)-0.5, edgecolor='black', rwidth=0.8)\n\n# Ajustar eixos e rótulos\nplt.xticks(range(len(labels)), custom_labels, rotation=0)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Absolute Frequency\")\nplt.title(\"Class Distribution for Dataset II\")\n\nplt.grid(axis='y', linestyle='--', alpha=0.6)\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# Input Dataset III","metadata":{}},{"cell_type":"code","source":"\n\nfor i in range(1, 50):\n    globals()[f\"data_normal_{i}\"] = pd.read_csv(f'/kaggle/input/comfaulda/COMFAULDA_v2/normal/normal_{i}.csv', header=None, sep = '[;,]', engine = 'python')\n\nfor i in range(1, 49):\n    globals()[f\"data_unbalance_i_{i}\"] = pd.read_csv(f'/kaggle/input/comfaulda/COMFAULDA_v2/unbalance_i/unbalance_6_{i}.csv', header=None, sep = '[;,]', engine = 'python')\n\nfor i in range(1, 49):\n    globals()[f\"data_unbalance_ii_{i}\"] = pd.read_csv(f'/kaggle/input/comfaulda/COMFAULDA_v2/unbalance_ii/unbalance_20_{i}.csv', header=None, sep = '[;,]', engine = 'python')\n\nfor i in range(1, 49):\n    globals()[f\"data_unbalance_iii_{i}\"] = pd.read_csv(f'/kaggle/input/comfaulda/COMFAULDA_v2/unbalance_iii/unbalance_35_{i}.csv', header=None, sep = '[;,]', engine = 'python')\n\n\nfor i in range(1, 50):\n    globals()[f\"data_misalignment_{i}\"] = pd.read_csv(f'/kaggle/input/comfaulda/COMFAULDA_v2/misalignment/misalignment_{i}.csv', header=None,sep = '[;,]', engine = 'python')\n\n\nfor i in range(1, 40):\n    globals()[f\"data_unbalance_misaligment_{i}\"] = pd.read_csv(f'/kaggle/input/comfaulda/COMFAULDA_v2/unbalance_misalignment/unbalance_misalignment_{i}.csv', header=None, sep = '[;,]', engine = 'python')\n\n\n\n\nprint('DATASET LOADED')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize which axis exhibits the highest amplitude.\n\n\ndf = globals()[\"data_unbalance_ii_42\"]\n\naxis_time = df.iloc[:, 0]\naxis_x = df.iloc[:, 5]\naxis_y = df.iloc[:, 7]\naxis_z = df.iloc[:, 6]\n\nplt.figure(figsize=(12, 6))\nplt.plot(time, axis_x, label='X axis')\nplt.plot(time, axis_y, label='Y axis')\nplt.plot(time, axis_z, label='Z axis')\nplt.title('Vibration in 3 axis - Unbalance II')\nplt.xlabel('Time (s)')\nplt.ylabel('Amplitude')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fs = 50000\n\nwindow_time = 1\nwindow = fs * window_time ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extracts signal segments of the specified window size.\n\ndef get_features(data, label):\n    n = int(np.floor(len(data)/window))\n    data = data[:int(n)*window]\n    X = data.values.reshape((n, window))\n    y = np.ones(n)*labels[label]\n    return X,y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = {'normal':0, 'unbalance_1':1, 'unbalance_2':2, 'unbalance_3':3, 'misaligment':4, 'unbalance_misaligment':5}\naxis = 6\n\n\nX_list = []\nY_list = []\n\n\nfor i in range(1, 50):\n    globals()[f'X_normal_{i}'], globals()[f'y_normal_{i}'] = get_features(globals()[f'data_normal_{i}'][axis], \"normal\")\n    X_list.append(globals()[f\"X_normal_{i}\"])\n    Y_list.append(globals()[f\"y_normal_{i}\"])\n\nfor i in range(1, 49):\n    globals()[f'X_unbalance_i_{i}'], globals()[f'y_unbalance_i_{i}'] = get_features(globals()[f'data_unbalance_i_{i}'][axis], \"unbalance_1\")\n    X_list.append(globals()[f\"X_unbalance_i_{i}\"])\n    Y_list.append(globals()[f\"y_unbalance_i_{i}\"])\n    \nfor i in range(1, 49):\n    globals()[f'X_unbalance_ii_{i}'], globals()[f'y_unbalance_ii_{i}'] = get_features(globals()[f'data_unbalance_ii_{i}'][axis], \"unbalance_2\")\n    X_list.append(globals()[f\"X_unbalance_ii_{i}\"])\n    Y_list.append(globals()[f\"y_unbalance_ii_{i}\"])\n\nfor i in range(1, 49):\n    globals()[f'X_unbalance_iii_{i}'], globals()[f'y_unbalance_iii_{i}'] = get_features(globals()[f'data_unbalance_iii_{i}'][axis], \"unbalance_3\")\n    X_list.append(globals()[f\"X_unbalance_iii_{i}\"])\n    Y_list.append(globals()[f\"y_unbalance_iii_{i}\"])\n\n\nfor i in range(1, 50):\n    globals()[f'X_misalignment_{i}'], globals()[f'y_misalignment_{i}'] = get_features(globals()[f'data_misalignment_{i}'][axis], \"misaligment\")\n    X_list.append(globals()[f\"X_misalignment_{i}\"])\n    Y_list.append(globals()[f\"y_misalignment_{i}\"])\n\nfor i in range(1, 40):\n    globals()[f'X_unbalance_misaligment_{i}'], globals()[f'y_unbalance_misaligment_{i}'] = get_features(globals()[f'data_unbalance_misaligment_{i}'][axis], \"unbalance_misaligment\")\n    X_list.append(globals()[f\"X_unbalance_misaligment_{i}\"])\n    Y_list.append(globals()[f\"y_unbalance_misaligment_{i}\"])\n\n\nX=np.concatenate(X_list)\nY=np.concatenate(Y_list)\n\n\nprint(X.shape, Y.shape)\n\nprint('Done')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Histogram used to visualize the number of samples per class (to verify whether the dataset is balanced).\n\ncustom_labels = [\n    \"Normal\",\n    \"Unb. I\",\n    \"Unb. II\",\n    \"Unb. III\",\n    \"Misalig.\",\n    \"Unb. II + Misalig.\"\n]\n\n# Criar histograma\nplt.figure(figsize=(8,5))\nplt.hist(Y, bins=np.arange(len(labels)+1)-0.5, edgecolor='black', rwidth=0.8)\n\n# Ajustar eixos e rótulos\nplt.xticks(range(len(labels)), custom_labels, rotation=0)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Absolute Frequency\")\nplt.title(\"Class Distribution for Dataset III\")\n\nplt.grid(axis='y', linestyle='--', alpha=0.6)\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"## Train, validation and test dataset Split","metadata":{}},{"cell_type":"markdown","source":"##### 70 % Train, 10% Validation and  20% Test","metadata":{}},{"cell_type":"code","source":"X, Y = shuffle(X, Y, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 20% of the data used for testing.\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n# 10% of the data used for validation.\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.125, random_state=42)\n\nprint(X_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape, Y_test.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CNN 1D - TIME DOMAIN","metadata":{}},{"cell_type":"code","source":"#Feature Scaling - Samples Standardization\n\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = X_train[..., np.newaxis]\nX_val   = X_val[...,   np.newaxis]\nX_test  = X_test[...,  np.newaxis] ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nstart_time = time.time()\n\nnum_classes = len(np.unique(Y_train)) \n\n\n# --- CNN model arquitecture ---\n\ninput_shape = X_train.shape[1:]\n\nmodel = Sequential([\n    Conv1D(32, kernel_size=3 , activation='relu', input_shape=input_shape),\n    MaxPooling1D(pool_size=2),\n    BatchNormalization(),\n    \n    Conv1D(64, kernel_size=3, activation='relu'),\n    MaxPooling1D(pool_size=2),\n    BatchNormalization(),\n    \n    Conv1D(128, kernel_size=3, activation='relu'),\n    MaxPooling1D(pool_size=2),\n    BatchNormalization(),\n    \n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(num_classes, activation='softmax')\n]) \n\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n\n\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy', \n              metrics=['accuracy']) \n\n\nmodel.summary()\n\n\n\n# --- Model training ---\nhistory = model.fit(X_train, Y_train, \n                    validation_data=(X_val, Y_val),\n                    epochs=250, \n                    batch_size=32,\n                    shuffle = True,\n                    callbacks=[early_stopping])\n\n\n\nend_time = time.time()\nelapsed_time = end_time - start_time\n\nprint(\"Time spent in training the model (s):\", elapsed_time)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Model evaluation ---\n\nY_pred = np.argmax(model.predict(X_test), axis=1)\nprint(Y_pred)\n\n\nprint(classification_report(Y_test, Y_pred))\n\n\ntest_loss, test_acc = model.evaluate(X_test, Y_test, verbose=0)\nprint(f\"Test Loss: {test_loss:.4f} - Test Accuracy: {test_acc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Learning curves.\n\nplt.plot(history.history['accuracy'], label='Test set')\nplt.plot(history.history['val_accuracy'], label = 'Validation set')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title(\"Accuracy curves\")\nplt.ylim([0, 1])\nplt.xlim([0, 250])\nplt.legend(loc='lower right')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conf_matriz = confusion_matrix(Y_test, Y_pred)\n\n#conf_matriz_classes = ['Normal', 'Unb. I', 'Unb. II', 'Unb. III', 'Unb. IV']\n#conf_matriz_classes = ['Normal', 'Unb. I', 'Unb. II', 'Misalig.', 'Bearings']\nconf_matriz_classes = ['Normal', 'Unb. I', 'Unb. II','Unb. III', 'Misalig.', 'Unb. II + Misalig.']\n\nplt.figure(figsize=(6, 5))\nsns.heatmap(conf_matriz, annot=True, fmt='d', cmap='Blues',\n            xticklabels=conf_matriz_classes, yticklabels=conf_matriz_classes,cbar=True)\n\nplt.title('Confusion matrix')\nplt.xlabel('Predicted Class')\nplt.ylabel('Real Class')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ROC Curve and AUC value\n\n#Binarize the labels.\nclasses = list(labels.values())  # [0, 1, 2, 3, 4]\nY_test_bin = label_binarize(Y_test, classes=classes)\n\n\nY_score = model.predict(X_test)\n\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n\nn_classes = len(classes)\n\n\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(Y_test_bin[:, i], Y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n\n\nplt.figure(figsize=(8, 6))\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'darkgreen', 'crimson','purple'])\n\n#class_names = ['Normal', 'Unb. I', 'Unb. II', 'Unb. III', 'Unb. IV']\n#class_names = ['Normal', 'Unb. I', 'Unb. II', 'Misalig.', 'Bearings']\nclass_names = ['Normal', 'Unb. I', 'Unb. II','Unb. III', 'Misalig.', 'Unb. II + Misalig.']\n\n\nfor i, (color, name) in enumerate(zip(colors, class_names)):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n             label=f\"{name} (AUC = {roc_auc[i]:.2f})\")\n\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Specificity)')\nplt.title('ROC Curves')\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Save model\n\nmodel.save(\"\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CNN 1D - FREQUENCY DOMAIN","metadata":{}},{"cell_type":"code","source":"def extract_features_fft(signal):\n    fft_vals = fft(signal)\n    fft_magnitude = np.abs(fft_vals)[:len(signal)//2]\n    \n    return fft_magnitude","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_fft = []\nX_val_fft = []\nX_test_fft = []\n\nfor signal in X_train:\n    featured_signal = extract_features_fft(signal)\n    X_train_fft.append(featured_signal)\n\nfor signal in X_val:\n    featured_signal = extract_features_fft(signal)\n    X_val_fft.append(featured_signal)\n    \nfor signal in X_test:\n    featured_signal = extract_features_fft(signal)\n    X_test_fft.append(featured_signal)    \n    \nX_train_fft =  np.array(X_train_fft)\nX_val_fft = np.array(X_val_fft)\nX_test_fft = np.array(X_test_fft)\n\n\nprint(X_train_fft.shape)\nprint(X_val_fft.shape)\nprint(X_test_fft.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_fft = X_train_fft[..., np.newaxis]\nX_val_fft   = X_val_fft[...,   np.newaxis]\nX_test_fft  = X_test_fft[...,  np.newaxis] ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\n\nnum_classes = len(np.unique(Y_train)) \n\n\n# --- CNN model arquitecture ---\ninput_shape = X_train_fft.shape[1:]\n\nmodel = Sequential([\n    Conv1D(32, kernel_size=3 , activation='relu', input_shape=input_shape),\n    MaxPooling1D(pool_size=2),\n    BatchNormalization(),\n    \n    Conv1D(64, kernel_size=3, activation='relu'),\n    MaxPooling1D(pool_size=2),\n    BatchNormalization(),\n    \n    Conv1D(128, kernel_size=3, activation='relu'),\n    MaxPooling1D(pool_size=2),\n    BatchNormalization(),\n    \n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(num_classes, activation='softmax')\n]) \n                 \nearly_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n\nmodel.summary()\n\n\n# --- Model training ---\nhistory = model.fit(X_train_fft, Y_train, \n                    validation_data=(X_val_fft, Y_val),\n                    epochs=250, \n                    batch_size=32,\n                    shuffle = True,\n                    callbacks=[early_stopping])\n\n\nend_time = time.time()\nelapsed_time = end_time - start_time\n\nprint(\"Time spent in training the model (s):\", elapsed_time)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Model evaluation ---\n\nY_pred = np.argmax(model.predict(X_test_fft), axis=1)\nprint(Y_pred)\n\n\nprint(classification_report(Y_test, Y_pred))\n\n\ntest_loss, test_acc = model.evaluate(X_test_fft, Y_test, verbose=0)\nprint(f\"Test Loss: {test_loss:.4f} - Test Accuracy: {test_acc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Learning curves.\n\nplt.plot(history.history['accuracy'], label='Test set')\nplt.plot(history.history['val_accuracy'], label = 'Validation set')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title(\"Accuracy curves\")\nplt.ylim([0, 1])\nplt.xlim([0, 60])\nplt.legend(loc='lower right')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conf_matriz = confusion_matrix(Y_test, Y_pred)\n\n#conf_matriz_classes = ['Normal', 'Unb. I', 'Unb. II', 'Unb. III', 'Unb. IV']\n#conf_matriz_classes = ['No unb.', 'Unb. I', 'Unb. II', 'Misalig.', 'Bearings']\nconf_matriz_classes = ['Normal', 'Unb. I', 'Unb. II','Unb. III', 'Misalig.', 'Unb. II + Misalig.']\n\nplt.figure(figsize=(6, 5))\nsns.heatmap(conf_matriz, annot=True, fmt='d', cmap='Blues',\n            xticklabels=conf_matriz_classes, yticklabels=conf_matriz_classes,cbar=True)\n\nplt.title('Confusion matrix')\nplt.xlabel('Predicted Class')\nplt.ylabel('Real Class')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ROC Curve and AUC value\n\n#Binarize the labels.\nclasses = list(labels.values())  # [0, 1, 2, 3, 4]\nY_test_bin = label_binarize(Y_test, classes=classes)\n\nY_score = model.predict(X_test_fft)\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n\nn_classes = len(classes)\n\n\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(Y_test_bin[:, i], Y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n\n\nplt.figure(figsize=(8, 6))\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'darkgreen', 'crimson'])\n\n\n#class_names = ['Normal', 'Unb. I', 'Unb. II', 'Unb. III', 'Unb. IV']\nclass_names = ['Normal', 'Unb. I', 'Unb. II','Unb. III', 'Misalig.', 'Unb. II + Misalig.']\n\n\nfor i, (color, name) in enumerate(zip(colors, class_names)):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n             label=f\"{name} (AUC = {roc_auc[i]:.2f})\")\n\n\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Specificity)')\nplt.title('ROC Curves')\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Save model\n\nmodel.save(\"\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CNN 2D - TIME DOMAIN","metadata":{}},{"cell_type":"code","source":"#Feature Scaling - Samples Standardization\n\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating figures from the time domain samples.\n\n\ndef segments_to_waveform_images(X, img_size=(128, 128)):\n   \n    n_segments, n_samples = X.shape\n    images = np.zeros((n_segments, img_size[0], img_size[1], 3), dtype=np.float32)\n\n    fig = plt.figure(figsize=(img_size[1]/100, img_size[0]/100), dpi=100)\n    ax = fig.add_subplot(111)\n    ax.set_axis_off()\n    \n    for i in range(n_segments):\n        ax.clear()\n        ax.plot(X[i], color='black', linewidth=1)\n        ax.set_xlim(0, n_samples-1)\n        ax.set_ylim(np.min(X[i]), np.max(X[i]))\n        ax.set_axis_off()\n        \n        canvas = FigureCanvasAgg(fig)\n        canvas.draw()\n        buf = canvas.buffer_rgba()\n        img = np.asarray(buf)[:, :, :3]\n        \n        img_pil = Image.fromarray(img)\n        img_resized = img_pil.resize(img_size, resample=Image.BILINEAR)\n        \n        images[i] = np.array(img_resized, dtype=np.float32) / 255.0\n    \n    plt.close(fig)\n    return images\n\n\nimg_size = (128, 128)\n\nX_train_img = segments_to_waveform_images(X_train, img_size)\nX_val_img   = segments_to_waveform_images(X_val,   img_size)\nX_test_img  = segments_to_waveform_images(X_test,  img_size)\n\nprint(\"Train images:\", X_train_img.shape)  # deve ser (n_train, 128, 128, 3)\nprint(\"Valdation images:\", X_val_img.shape)\nprint(\"Test images:\", X_test_img.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example of one sample.\n\nimg = 30\n\nplt.imshow(X_train_img[img])\nplt.axis(\"off\")\nplt.show()\n\nprint(Y_train[img])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\n\nnum_classes = len(np.unique(Y_train)) \n\n\n# --- CNN model arquitecture ---\ninput_shape = X_train_img.shape[1:]\n\nmodel = Sequential([\n    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n    MaxPooling2D(pool_size=(2, 2)),\n    BatchNormalization(),\n    \n    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    BatchNormalization(),\n    \n    Conv2D(128, kernel_size=(3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    BatchNormalization(),\n    \n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(num_classes, activation='softmax')      \n])\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n\nmodel.summary()\n\n\n# --- Model training ---\nhistory = model.fit(X_train_img, Y_train, \n                    validation_data=(X_val_img, Y_val),\n                    epochs=250, \n                    batch_size=32,\n                    shuffle = True,\n                    callbacks=[early_stopping])\n\n\n\nend_time = time.time()\nelapsed_time = end_time - start_time\n\nprint(\"Time spent in training the model (s):\", elapsed_time)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Model evaluation ---\n\nY_pred = np.argmax(model.predict(X_test_img), axis=1)\nprint(Y_pred)\n\n\nprint(classification_report(Y_test, Y_pred))\n\n\ntest_loss, test_acc = model.evaluate(X_test_img, Y_test, verbose=0)\nprint(f\"Test Loss: {test_loss:.4f} - Test Accuracy: {test_acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Learning curves.\n\nplt.plot(history.history['accuracy'], label='Test set')\nplt.plot(history.history['val_accuracy'], label = 'Validation set')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title(\"Accuracy curves\")\nplt.ylim([0, 1])\nplt.xlim([0, 50])\nplt.legend(loc='lower right')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conf_matriz = confusion_matrix(Y_test, Y_pred)\n\nconf_matriz_classes = ['Normal', 'Unb. I', 'Unb. II', 'Unb. III', 'Unb. IV']\n# conf_matriz_classes = ['No unb.', 'Unb. I', 'Unb. II', 'Misalig.', 'Bearings']\n#conf_matriz_classes = ['Normal', 'Unb. I', 'Unb. II','Unb. III', 'Misalig.', 'Unb. II + Misalig.']\n\nplt.figure(figsize=(6, 5))\nsns.heatmap(conf_matriz, annot=True, fmt='d', cmap='Blues',\n            xticklabels=conf_matriz_classes, yticklabels=conf_matriz_classes,cbar=True)\n\nplt.title('Confusion matrix')\nplt.xlabel('Predicted Class')\nplt.ylabel('Real Class')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ROC Curve and AUC value\n\nclasses = list(labels.values())\nY_test_bin = label_binarize(Y_test, classes=classes)\n\nY_score = model.predict(X_test_img)\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n\nn_classes = len(classes)\n\n\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(Y_test_bin[:, i], Y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n\nplt.figure(figsize=(8, 6))\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'darkgreen', 'crimson'])\n\nclass_names = ['Normal', 'Unb. I', 'Unb. II', 'Unb. III', 'Unb. IV']\n#class_names =  = ['Normal', 'Unb. I', 'Unb. II','Unb. III', 'Misalig.', 'Unb. II + Misalig.']\n\n\nfor i, (color, name) in enumerate(zip(colors, class_names)):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n             label=f\"{name} (AUC = {roc_auc[i]:.2f})\")\n\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Specificity)')\nplt.title('ROC Curves')\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Save model\n\nmodel.save(\"\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CNN 2D - FREQUENCY DOMAIN","metadata":{}},{"cell_type":"code","source":"# Creating the spectogram of each sample.\n\n\ndef compute_spectrogram(dataset, fs):\n    spectrograms = []\n    epsilon = 1e-10\n    \n    for n in range(dataset.shape[0]):\n        Pxx, freqs, bins, im = plt.specgram(dataset[n], NFFT=256, Fs=fs, cmap='inferno')\n        plt.close()\n        \n        Pxx_db = 10 * np.log10(Pxx + epsilon)\n        \n        Pxx_norm = (Pxx_db - np.min(Pxx_db)) / (np.max(Pxx_db) - np.min(Pxx_db))\n        \n        spectrograms.append(Pxx_norm)\n    \n    spectrograms = np.array(spectrograms)\n    return spectrograms\n\n\nX_train_spec = compute_spectrogram(X_train,fs)\nX_val_spec = compute_spectrogram(X_val,fs)\nX_test_spec = compute_spectrogram(X_test,fs)\n\n\nprint(\"Spectrogram array format X_train_spec:\", X_train_spec.shape)\nprint(\"Spectrogram array format X_val_spec:\", X_val_spec.shape)\nprint(\"Spectrogram array format X_test_spec:\", X_test_spec.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef compute_spectrogram(dataset, fs):\n    target_fs = 4096.0\n    spectrograms = []\n    epsilon = 1e-10\n\n\n    do_resample = (float(fs) != target_fs)\n    if do_resample:\n        cutoff = 0.9 * (target_fs * 0.5)\n        nyq_orig = 0.5 * fs\n        norm_cut = min(0.999, max(1e-6, cutoff / nyq_orig))\n        b, a = butter(6, norm_cut, btype='low')\n        frac = Fraction(int(target_fs), int(fs)).limit_denominator(1000)\n        up, down = frac.numerator, frac.denominator\n    else:\n        b = a = None\n        up = down = None\n\n    for n in range(dataset.shape[0]):\n        x = dataset[n].astype(np.float64, copy=False)\n\n        if do_resample:\n            x = filtfilt(b, a, x)\n            x = resample_poly(x, up, down)\n\n        Pxx, freqs, bins, im = plt.specgram(x, NFFT=256, Fs=target_fs, cmap='inferno')\n        plt.close()\n\n        Pxx_db = 10.0 * np.log10(Pxx + epsilon)\n        Pxx_norm = (Pxx_db - np.min(Pxx_db)) / (np.max(Pxx_db) - np.min(Pxx_db) + epsilon)\n\n        spectrograms.append(Pxx_norm.astype(np.float32))\n\n    return np.array(spectrograms)\n\n\nX_train_spec = compute_spectrogram(X_train,fs)\nX_val_spec = compute_spectrogram(X_val,fs)\nX_test_spec = compute_spectrogram(X_test,fs)\n\n\nprint(\"Spectrogram array format X_train_spec:\", X_train_spec.shape)\nprint(\"Spectrogram array format X_val_spec:\", X_val_spec.shape)\nprint(\"Spectrogram array format X_test_spec:\", X_test_spec.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example of one spectogram\n\nimg = 15\n\nplt.imshow(X_train_spec[img])\nplt.axis(\"off\")\nplt.show()\n\nprint(Y_train[img])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example of one spectogram\n\nimg = 15\nx_vis = X_train[img]\nPxx_, freqs, bins, _ = plt.specgram(x_vis, NFFT=256, Fs=4096)\nplt.close()\n\nextent = [bins[0], bins[-1], freqs[0], freqs[-1]]\n\nplt.figure(figsize=(10, 3))\nplt.imshow(X_train_spec[img], origin='lower', aspect='auto', extent=extent, cmap='inferno')\nplt.xlabel('Tempo [s]')\nplt.ylabel('Frequência [Hz]')\nplt.colorbar(label='(normalizado)')\nplt.show()\n\nprint(Y_train[img])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_spec = X_train_spec[..., np.newaxis]\nX_val_spec   = X_val_spec[..., np.newaxis]\nX_test_spec  = X_test_spec[..., np.newaxis]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nstart_time = time.time()\n\nnum_classes = len(np.unique(Y_train)) \n\n\n# --- CNN model arquitecture ---\ninput_shape = X_train_spec.shape[1:]\n\nmodel = Sequential([\n    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n    MaxPooling2D(pool_size=(2, 2)),\n    BatchNormalization(),\n    \n    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    BatchNormalization(),\n    \n    Conv2D(128, kernel_size=(3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    BatchNormalization(),\n    \n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(num_classes, activation='softmax')\n])\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n\nmodel.summary()\n\n\n# --- Model training ---\nhistory = model.fit(X_train_spec, Y_train, \n                    validation_data=(X_val_spec, Y_val),\n                    epochs=250, \n                    batch_size=32,\n                    shuffle = True,\n                    callbacks=[early_stopping])\n\n\nend_time = time.time()\nelapsed_time = end_time - start_time\n\nprint(\"Time spent in training the model (s):\", elapsed_time)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Model evaluation ---\n\nY_pred = np.argmax(model.predict(X_test_spec), axis=1)\nprint(Y_pred)\n\n\nprint(classification_report(Y_test, Y_pred))\n\n\ntest_loss, test_acc = model.evaluate(X_test_spec, Y_test, verbose=0)\nprint(f\"Test Loss: {test_loss:.4f} - Test Accuracy: {test_acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Learning curves.\n\nplt.plot(history.history['accuracy'], label='Test set')\nplt.plot(history.history['val_accuracy'], label = 'Validation set')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title(\"Accuracy curves\")\nplt.ylim([0, 1])\nplt.xlim([0, 172])\nplt.legend(loc='lower right')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conf_matriz = confusion_matrix(Y_test, Y_pred)\n\n#conf_matriz_classes = ['Normal', 'Unb. I', 'Unb. II', 'Unb. III', 'Unb. IV']\n# conf_matriz_classes = ['No unb.', 'Unb. I', 'Unb. II', 'Misalig.', 'Bearings']\nconf_matriz_classes = ['Normal', 'Unb. I', 'Unb. II','Unb. III', 'Misalig.', 'Unb. II + Misalig.']\n\nplt.figure(figsize=(6, 5))\nsns.heatmap(conf_matriz, annot=True, fmt='d', cmap='Blues',\n            xticklabels=conf_matriz_classes, yticklabels=conf_matriz_classes,cbar=True)\n\nplt.title('Confusion matrix')\nplt.xlabel('Predicted Class')\nplt.ylabel('Real Class')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ROC Curve and AUC value\n\nclasses = list(labels.values())  # [0, 1, 2, 3, 4]\nY_test_bin = label_binarize(Y_test, classes=classes)\n\nY_score = model.predict(X_test_spec)\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n\nn_classes = len(classes)\n\n\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(Y_test_bin[:, i], Y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n\nplt.figure(figsize=(8, 6))\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'darkgreen', 'crimson'])\n\n#class_names = ['Normal', 'Unb. I', 'Unb. II', 'Unb. III', 'Unb. IV']\nclass_names =  ['Normal', 'Unb. I', 'Unb. II','Unb. III', 'Misalig.', 'Unb. II + Misalig.']\n\nfor i, (color, name) in enumerate(zip(colors, class_names)):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n             label=f\"{name} (AUC = {roc_auc[i]:.2f})\")\n\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Specificity)')\nplt.title('ROC Curves')\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CNN 1D - FREQUENCY DOMAIN (NOISY EXPERIMENTS)","metadata":{}},{"cell_type":"code","source":"def add_white_noise(signal, snr_db):\n    signal_power = np.mean(signal**2)\n    snr_linear = 10 ** (snr_db / 10)\n    noise_power = signal_power / snr_linear\n    noise = np.random.normal(0, np.sqrt(noise_power), signal.shape)\n    return signal + noise","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Run this to add noise only to the Testing and Validation dataset\n\nX_train_noisy = np.array([add_white_noise(signal, snr_db=10) for signal in X_train])\nX_val_noisy = np.array([add_white_noise(signal, snr_db=10) for signal in X_val]) \n\n# snr_db=30 → Almost noise-free.\n# snr_db=10 → Moderate noise.\n# snr_db=5 → High noise.\n\nprint(X_train_noisy.shape)\nprint(X_val_noisy.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run this to add noise only to the Testing dataset\n\nX_test_noisy = np.array([add_white_noise(signal, snr_db=10) for signal in X_test]) \n\n\nprint(X_test_noisy.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run this to add noise to all datasets\n\nX_train_noisy = np.array([add_white_noise(signal, snr_db=10) for signal in X_train])\nX_val_noisy = np.array([add_white_noise(signal, snr_db=10) for signal in X_val])\nX_test_noisy = np.array([add_white_noise(signal, snr_db=10) for signal in X_test]) \n\n\n\nprint(X_train_noisy.shape)\nprint(X_val_noisy.shape)\nprint(X_test_noisy.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### -----------------------------------------------------------------","metadata":{}},{"cell_type":"code","source":"def extract_features_fft(signal):\n    fft_vals = fft(signal)\n    fft_magnitude = np.abs(fft_vals)[:len(signal)//2]\n    \n    return fft_magnitude","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Adjust according the datasets used.\n\nX_train_fft = []\nX_val_fft = []\nX_test_fft = []\n\nfor signal in X_train_noisy:\n    featured_signal = extract_features_fft(signal)\n    X_train_fft.append(featured_signal)\n\nfor signal in X_val_noisy:\n    featured_signal = extract_features_fft(signal)\n    X_val_fft.append(featured_signal)\n    \nfor signal in X_test_noisy:\n    featured_signal = extract_features_fft(signal)\n    X_test_fft.append(featured_signal)    \n    \nX_train_fft =  np.array(X_train_fft)\nX_val_fft = np.array(X_val_fft)\nX_test_fft = np.array(X_test_fft)\n\n\nprint(X_train_fft.shape)\nprint(X_val_fft.shape)\nprint(X_test_fft.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_fft = X_train_fft[..., np.newaxis]\nX_val_fft   = X_val_fft[...,   np.newaxis]\nX_test_fft  = X_test_fft[...,  np.newaxis] ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\n\nnum_classes = len(np.unique(Y_train)) \n\n\n# --- CNN model arquitecture ---\ninput_shape = X_train_fft.shape[1:]\n\nmodel = Sequential([\n    Conv1D(32, kernel_size=3 , activation='relu', input_shape=input_shape),\n    MaxPooling1D(pool_size=2),\n    BatchNormalization(),\n    \n    Conv1D(64, kernel_size=3, activation='relu'),\n    MaxPooling1D(pool_size=2),\n    BatchNormalization(),\n    \n    Conv1D(128, kernel_size=3, activation='relu'),\n    MaxPooling1D(pool_size=2),\n    BatchNormalization(),\n    \n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(num_classes, activation='softmax')\n]) \n                 \nearly_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n\nmodel.summary()\n\n\n# --- Model training ---\nhistory = model.fit(X_train_fft, Y_train, \n                    validation_data=(X_val_fft, Y_val),\n                    epochs=250, \n                    batch_size=32,\n                    shuffle = True,\n                    callbacks=[early_stopping])\n\n\nend_time = time.time()\nelapsed_time = end_time - start_time\n\nprint(\"Time spent in training the model (s):\", elapsed_time)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Model evaluation ---\n\nY_pred = np.argmax(model.predict(X_test_fft), axis=1)\nprint(Y_pred)\n\n\nprint(classification_report(Y_test, Y_pred))\n\n\ntest_loss, test_acc = model.evaluate(X_test_fft, Y_test, verbose=0)\nprint(f\"Test Loss: {test_loss:.4f} - Test Accuracy: {test_acc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Learning curves.\n\nplt.plot(history.history['accuracy'], label='Test set')\nplt.plot(history.history['val_accuracy'], label = 'Validation set')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title(\"Accuracy curves\")\nplt.ylim([0, 1])\nplt.xlim([0, 50])\nplt.legend(loc='lower right')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conf_matriz = confusion_matrix(Y_test, Y_pred)\n\n#conf_matriz_classes = ['Normal', 'Unb. I', 'Unb. II', 'Unb. III', 'Unb. IV']\n#conf_matriz_classes = ['No unb.', 'Unb. I', 'Unb. II', 'Misalig.', 'Bearings']\nconf_matriz_classes = ['Normal', 'Unb. I', 'Unb. II','Unb. III', 'Misalig.', 'Unb. II + Misalig.']\n\nplt.figure(figsize=(6, 5))\nsns.heatmap(conf_matriz, annot=True, fmt='d', cmap='Blues',\n            xticklabels=conf_matriz_classes, yticklabels=conf_matriz_classes,cbar=True)\n\nplt.title('Confusion matrix')\nplt.xlabel('Predicted Class')\nplt.ylabel('Real Class')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ROC Curve and AUC value\n\nclasses = list(labels.values())  # [0, 1, 2, 3, 4]\nY_test_bin = label_binarize(Y_test, classes=classes)\n\nY_score = model.predict(X_test_fft)\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n\nn_classes = len(classes)\n\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(Y_test_bin[:, i], Y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nplt.figure(figsize=(8, 6))\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'darkgreen', 'crimson'])\n\n# Nomes personalizados das classes\n#class_names = ['Normal', 'Unb. I', 'Unb. II', 'Unb. III', 'Unb. IV']\nclass_names = ['Normal', 'Unb. I', 'Unb. II','Unb. III', 'Misalig.', 'Unb. II + Misalig.']\n\n\nfor i, (color, name) in enumerate(zip(colors, class_names)):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n             label=f\"{name} (AUC = {roc_auc[i]:.2f})\")\n\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Specificity)')\nplt.title('ROC Curves')\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CNN 2D - FREQUENCY DOMAIN (NOISY EXPERIMENTS)","metadata":{}},{"cell_type":"code","source":"def add_white_noise(signal, snr_db):\n    signal_power = np.mean(signal**2)\n    snr_linear = 10 ** (snr_db / 10)\n    noise_power = signal_power / snr_linear\n    noise = np.random.normal(0, np.sqrt(noise_power), signal.shape)\n    return signal + noise","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Run this to add noise only to the Testing and Validation dataset\n\nX_train_noisy = np.array([add_white_noise(signal, snr_db=10) for signal in X_train])\nX_val_noisy = np.array([add_white_noise(signal, snr_db=10) for signal in X_val]) \n\n\nprint(X_train_noisy.shape)\nprint(X_val_noisy.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run this to add noise only to the Testing dataset\n\nX_test_noisy = np.array([add_white_noise(signal, snr_db=10) for signal in X_test]) \n\n\nprint(X_test_noisy.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run this to add noise to all datasets\n\nX_train_noisy = np.array([add_white_noise(signal, snr_db=10) for signal in X_train])\nX_val_noisy = np.array([add_white_noise(signal, snr_db=10) for signal in X_val])\nX_test_noisy = np.array([add_white_noise(signal, snr_db=10) for signal in X_test]) \n\n\nprint(X_train_noisy.shape)\nprint(X_val_noisy.shape)\nprint(X_test_noisy.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ---------------------------------------------------------","metadata":{}},{"cell_type":"code","source":"def compute_spectrogram(dataset, fs):\n    target_fs = 4096.0\n    spectrograms = []\n    epsilon = 1e-10\n\n    do_resample = (float(fs) != target_fs)\n    if do_resample:\n\n        cutoff = 0.9 * (target_fs * 0.5)\n        nyq_orig = 0.5 * fs\n        norm_cut = min(0.999, max(1e-6, cutoff / nyq_orig))\n        b, a = butter(6, norm_cut, btype='low')\n        frac = Fraction(int(target_fs), int(fs)).limit_denominator(1000)\n        up, down = frac.numerator, frac.denominator\n    else:\n        b = a = None\n        up = down = None\n\n    for n in range(dataset.shape[0]):\n        x = dataset[n].astype(np.float64, copy=False)\n\n        if do_resample:\n            x = filtfilt(b, a, x)\n            x = resample_poly(x, up, down)\n\n        Pxx, freqs, bins, im = plt.specgram(x, NFFT=256, Fs=target_fs, cmap='inferno')\n        plt.close()\n\n        Pxx_db = 10.0 * np.log10(Pxx + epsilon)\n        Pxx_norm = (Pxx_db - np.min(Pxx_db)) / (np.max(Pxx_db) - np.min(Pxx_db) + epsilon)\n\n        spectrograms.append(Pxx_norm.astype(np.float32))\n\n    return np.array(spectrograms)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adjust according the datasets used.\n\nX_train_spec = compute_spectrogram(X_train_noisy,fs)\nX_val_spec = compute_spectrogram(X_val_noisy,fs)\nX_test_spec = compute_spectrogram(X_test_noisy,fs)\n\n\nprint(\"Spectrogram array format X_train_spec:\", X_train_spec.shape)\nprint(\"Spectrogram array format X_val_spec:\", X_val_spec.shape)\nprint(\"Spectrogram array format X_test_spec:\", X_test_spec.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example of one spectogram\n\nimg = 15\n\nplt.imshow(X_train_spec[img])\nplt.axis(\"off\")\nplt.show()\n\nprint(Y_train[img])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_spec = X_train_spec[..., np.newaxis]\nX_val_spec   = X_val_spec[..., np.newaxis]\nX_test_spec  = X_test_spec[..., np.newaxis]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\n\nnum_classes = len(np.unique(Y_train)) \n\n\n# --- CNN model arquitecture ---\ninput_shape = X_train_spec.shape[1:]\n\nmodel = Sequential([\n    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n    MaxPooling2D(pool_size=(2, 2)),\n    BatchNormalization(),\n    \n    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    BatchNormalization(),\n    \n    Conv2D(128, kernel_size=(3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    BatchNormalization(),\n    \n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(num_classes, activation='softmax')\n])\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()\n\n\n# --- Model training ---\nhistory = model.fit(X_train_spec, Y_train, \n                    validation_data=(X_val_spec, Y_val),\n                    epochs=250, \n                    batch_size=32,\n                    shuffle = True,\n                    callbacks=[early_stopping])\n\n\nend_time = time.time()\nelapsed_time = end_time - start_time\n\nprint(\"Time spent in training the model (s):\", elapsed_time)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Model evaluation ---\n\nY_pred = np.argmax(model.predict(X_test_spec), axis=1)\nprint(Y_pred)\n\n\nprint(classification_report(Y_test, Y_pred))\n\n\ntest_loss, test_acc = model.evaluate(X_test_spec, Y_test, verbose=0)\nprint(f\"Test Loss: {test_loss:.4f} - Test Accuracy: {test_acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Learning curves.\n\nplt.plot(history.history['accuracy'], label='Test set')\nplt.plot(history.history['val_accuracy'], label = 'Validation set')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title(\"Accuracy curves\")\nplt.ylim([0, 1])\nplt.xlim([0, 50])\nplt.legend(loc='lower right')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conf_matriz = confusion_matrix(Y_test, Y_pred)\n\n#conf_matriz_classes = ['Normal', 'Unb. I', 'Unb. II', 'Unb. III', 'Unb. IV']\n#conf_matriz_classes = ['No unb.', 'Unb. I', 'Unb. II', 'Misalig.', 'Bearings']\nconf_matriz_classes = ['Normal', 'Unb. I', 'Unb. II','Unb. III', 'Misalig.', 'Unb. II + Misalig.']\n\nplt.figure(figsize=(6, 5))\nsns.heatmap(conf_matriz, annot=True, fmt='d', cmap='Blues',\n            xticklabels=conf_matriz_classes, yticklabels=conf_matriz_classes,cbar=True)\n\nplt.title('Confusion matrix')\nplt.xlabel('Predicted Class')\nplt.ylabel('Real Class')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ROC Curve and AUC value\n\nclasses = list(labels.values())\nY_test_bin = label_binarize(Y_test, classes=classes)\n\n\nY_score = model.predict(X_test_fft)\n\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n\nn_classes = len(classes)\n\n\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(Y_test_bin[:, i], Y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n\nplt.figure(figsize=(8, 6))\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'darkgreen', 'crimson'])\n\n\n#class_names = ['Normal', 'Unb. I', 'Unb. II', 'Unb. III', 'Unb. IV']\nclass_names = ['Normal', 'Unb. I', 'Unb. II','Unb. III', 'Misalig.', 'Unb. II + Misalig.']\n\n\nfor i, (color, name) in enumerate(zip(colors, class_names)):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n             label=f\"{name} (AUC = {roc_auc[i]:.2f})\")\n\n\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Specificity)')\nplt.title('ROC Curves')\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
